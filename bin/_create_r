# create cluster
aws emr create-cluster --name "$CLUSTER_NAME" --region $AWS_REGION \
    --release-label $AWS_EMR_LABEL \
    --applications Name=Ganglia Name=Spark \
    --service-role EMR_DefaultRole \
    --instance-groups \
    InstanceGroupType=MASTER,InstanceType=$EC2_TYPE,InstanceCount=1 \
    InstanceGroupType=CORE,InstanceType=$EC2_TYPE,InstanceCount=1 \
    InstanceGroupType=TASK,InstanceType=$EC2_TYPE,InstanceCount=$NUM_TASK_INSTANCE,BidPrice=$TASK_SPOT_BID_PRICE \
    --configurations '[{"Classification":"spark","Properties":{"maximizeResourceAllocation":"true"},"Configurations":[]}]' \
    --bootstrap-actions Name=Init_for_Wadal,Path=$INIT_SCRIPT_DIR_S3/init_r.sh \
    --ec2-attributes \
KeyName=$EC2_KEY_PAIR_NAME,\
InstanceProfile=EMR_EC2_DefaultRole,\
SubnetId=$AWS_EMR_SUBNET\
    --steps Type=CUSTOM_JAR,Name=CopyCSVLib,ActionOnFailure=CANCEL_AND_WAIT,Jar=s3://$AWS_REGION.elasticmapreduce/libs/script-runner/script-runner.jar,Args=["$INIT_SCRIPT_DIR_S3/cp_csvlib.sh"] Type=CUSTOM_JAR,Name=RunJupyter,ActionOnFailure=CANCEL_AND_WAIT,Jar=s3://$AWS_REGION.elasticmapreduce/libs/script-runner/script-runner.jar,Args=["$INIT_SCRIPT_DIR_S3/run_rstudio.sh","$AWS_S3_ACCESS_KEY","$AWS_S3_SECRET_KEY","$NOTEBOOK_S3_BUCKET"] | sed -n '/ClusterId/p' | cut -d':' -f2 | tr -d ' "' > "clusters/$1_cluster.id"
